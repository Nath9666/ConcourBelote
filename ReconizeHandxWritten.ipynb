{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance de Chiffres Manuscrits avec SVHN\n",
    "\n",
    "Ce notebook utilise le dataset SVHN (Street View House Numbers) pour entraîner un modèle de reconnaissance de chiffres manuscrits. Nous allons charger les données, les prétraiter, construire un modèle de réseau de neurones convolutif, l'entraîner et l'évaluer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des Bibliothèques\n",
    "\n",
    "Nous commençons par importer les bibliothèques nécessaires pour notre projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et Prétraitement des Données\n",
    "\n",
    "Nous chargeons le dataset SVHN et appliquons un prétraitement pour normaliser les images et encoder les étiquettes en one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Nathan\\tensorflow_datasets\\svhn_cropped\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...:  67%|██████▋   | 2/3 [01:15<00:37, 37.77s/ url]"
     ]
    }
   ],
   "source": [
    "# Chargement du dataset SVHN\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'svhn_cropped',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,  # Retourne un tuple (img, label) au lieu d'un dictionnaire {'image': img, 'label': label}\n",
    "    with_info=True  # Retourne le tuple avec les informations du dataset\n",
    ")\n",
    "\n",
    "def preprocess(images, labels):\n",
    "    # Normalisation des images\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    # One-hot encoding des étiquettes\n",
    "    labels = tf.one_hot(labels, depth=10)\n",
    "    return images, labels\n",
    "\n",
    "# Prétraitement des données\n",
    "ds_train = ds_train.map(preprocess).batch(32)\n",
    "ds_test = ds_test.map(preprocess).batch(32)\n",
    "\n",
    "# Taille des images dans SVHN\n",
    "hauteur, largeur, canaux = ds_info.features['image'].shape\n",
    "num_classes = ds_info.features['label'].num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du Modèle\n",
    "\n",
    "Nous construisons un modèle de réseau de neurones convolutif pour la reconnaissance de séquences de chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle adapté à la reconnaissance de séquences de chiffres\n",
    "model = models.Sequential()\n",
    "\n",
    "# Base convolutive pour l'extraction de caractéristiques\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(hauteur, largeur, canaux)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Adaptation pour la reconnaissance de séquences\n",
    "# Ici, vous devez ajuster `sequence_length` et `features` selon la sortie de votre couche convolutive\n",
    "model.add(layers.Flatten())  # Utilisez Flatten comme simplification pour cet exemple\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Couche de sortie pour la classification de chaque chiffre dans la séquence\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du Modèle\n",
    "\n",
    "Nous configurons l'early stopping pour arrêter l'entraînement si la perte de validation ne s'améliore pas après 3 époques, puis nous entraînons le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Surveille la perte de validation\n",
    "    patience=3,  # Nombre d'époques sans amélioration après lesquelles l'entraînement sera arrêté\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  # Restaure les meilleurs poids trouvés\n",
    ")\n",
    "\n",
    "# Entraînement du modèle\n",
    "historique = model.fit(\n",
    "    ds_train,\n",
    "    epochs=10,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[early_stopping]  # Ajoute l'Early Stopping à la liste des callbacks\n",
    ")\n",
    "\n",
    "#model.save('./models/svhn_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du Modèle\n",
    "\n",
    "Nous évaluons les performances du modèle sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8874 - loss: 0.3845\n",
      "Perte de test: 0.394876092672348\n",
      "Précision de test: 0.8884450197219849\n"
     ]
    }
   ],
   "source": [
    "# Évaluation du modèle sur l'ensemble de test\n",
    "test_loss, test_acc = model.evaluate(ds_test)\n",
    "\n",
    "print(f\"Perte de test: {test_loss}\")\n",
    "print(f\"Précision de test: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
